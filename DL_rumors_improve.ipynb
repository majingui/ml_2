{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 机器学习第二次作业——谣言检测\n",
    "\n",
    "马锦贵 学号 2401212867\n",
    "\n",
    "说明：按照要求，本次作业的报告直接和jupyter notebook合并，方便结合代码解释\n",
    "\n",
    "本次任务有两个部分：\n",
    "\n",
    "任务一：在原有代码上实现K-Fold验证，报告基于transformer方法的准确率\n",
    "\n",
    "任务二：以任务一为baseline，改进提升准确率"
   ],
   "metadata": {
    "id": "q_tTmBzBB-xY"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "B-T7bzGuZ0TF"
   },
   "source": [
    "# 任务一.\n",
    "**基于原有代码实现K-Fold交叉验证**，报告基于transformer方法的准确率\n",
    "\n",
    "\n",
    "**代码的核心改动在数据集加载和模型训练[在代码中标记为“K-Fold关键改动”]**\n",
    "\n",
    "**结论：**\n",
    "经过K-Fold验证，基于Transformer的方法的预测准确率**平均是86.81%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "w-jxVm_yZ0TI"
   },
   "source": [
    "## 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7FcVtSdZ0TI",
    "outputId": "8721c5bb-5480-46c9-f133-3bf8639413c0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (0.42.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "1e6kua-aZ0TJ",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import json, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#PyTorch用的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 自然语言处理相关的包\n",
    "import re #正则表达式的包\n",
    "import jieba #结巴分词包\n",
    "from collections import Counter #搜集器，可以让统计词频更简单\n",
    "\n",
    "#绘图、计算用的程序包\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "SJaS_pSWZ0TK"
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvC9KNybZ0TK",
    "outputId": "807eb721-17c8-4753-b82d-200cfe0dada3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'Chinese_Rumor_Dataset' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/thunlp/Chinese_Rumor_Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOdO6Z5XZ0TL",
    "outputId": "68643a28-3cde-4632-c36a-7c872553b8c3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CED_Dataset  README.md\trumors_v170613.json\n",
      "non-rumor-repost  original-microblog  README.md  rumor-repost\n"
     ]
    }
   ],
   "source": [
    "!ls ./Chinese_Rumor_Dataset\n",
    "!ls ./Chinese_Rumor_Dataset/CED_Dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "h470aVfAZ0TL"
   },
   "source": [
    "### 获取微博文本及其配对标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mXoCAGFZ0TM",
    "outputId": "47a622d2-c173-4164-b664-8a8ca5322270"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "开始读取数据\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3389/3389 [00:07<00:00, 464.28it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "结束, 有1538条谣言, 有1849条非谣言!\n",
      "['今晚研究百合网，婚恋网站，因为明天做会议主持要对话百合网老大。第一次注册，看到好多姑娘的照片，但是系统给我推荐的怎么全是30岁以上的姑娘呢？在百合上看到一个有意思的统计分析，见附图~，女人应该在25岁之前把自己嫁出去哦~', '【亲，清华食堂新增了热干面哟！】@长江日报 讯：为吸引湖北的优质生源，清华大学食堂推出热干面，据说口味正宗，很受欢迎，想吃得排20分钟队，供不应求。清华大学湖北招生组甚至微博卖萌“亲，刚刚新增了热干面哟~”网友称赞清华以神来之笔，在与北大PK“掐尖”中完胜。[哈哈]']\n",
      "--------------------\n",
      "['【新浪内部人控制的用户黑洞】新浪内部技术人员开发了一套微博平台，据称现在该平台拥有上亿用户，并且规模在不断增加。这些用户都是制造的，但可以被操纵去评论、投票等，内部人员利用这些业务让一级代理、二级代理去兜售，形成了特别利益链条。#钛爱拍# http://t.cn/zYC5lcZ', '美国高二的数学...请用一句话概况你的感受！']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据来源文件夹 -- 内含多个json文件\n",
    "non_rumor = './Chinese_Rumor_Dataset/CED_Dataset/non-rumor-repost'\n",
    "rumor = './Chinese_Rumor_Dataset/CED_Dataset/rumor-repost'\n",
    "original = './Chinese_Rumor_Dataset/CED_Dataset/original-microblog'\n",
    "\n",
    "non_rumor_data = []\n",
    "rumor_data = []\n",
    "\n",
    "# 遍历文件夹，读取文本数据\n",
    "print('开始读取数据')\n",
    "for file in tqdm(os.listdir(original)):\n",
    "    try:\n",
    "        data = json.load(open(os.path.join(original, file), 'rb'))['text']\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    is_rumor = (file in os.listdir(rumor))\n",
    "    if is_rumor:\n",
    "        rumor_data.append(data)\n",
    "    else:\n",
    "        non_rumor_data.append(data)\n",
    "\n",
    "print('结束, 有{}条谣言, 有{}条非谣言!'.format(len(rumor_data), len(non_rumor_data)))\n",
    "print(non_rumor_data[-2:])\n",
    "print('-'*20)\n",
    "print(rumor_data[-2:])\n",
    "\n",
    "\n",
    "# 把数据储存到指定地方 -- 统一到2个txt文件\n",
    "pth = './rumor_detection_data'\n",
    "if not os.path.exists(pth):\n",
    "    os.makedirs(pth)\n",
    "\n",
    "good_file = os.path.join(pth, 'non_rumor.txt')\n",
    "bad_file = os.path.join(pth, 'rumor.txt')\n",
    "\n",
    "with open(good_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(non_rumor_data))\n",
    "with open(bad_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(rumor_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "9VFN-lVbZ0TM"
   },
   "source": [
    "### 文本预处理（过滤标点，分词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpOY5dVaZ0TN",
    "outputId": "2bc9898c-cdcd-4600-ecbc-f8db2243d27b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "分词前： 请转发请让国人们都看看请转发请让某些人也看看某某人一定能看到\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.760 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.760 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "分词后： ['请', '转发', '请', '让', '国', '人们', '都', '看看', '请', '转发', '请', '让', '某些', '人', '也', '看看', '某某人', '一定', '能', '看到']\n",
      "./rumor_detection_data/non_rumor.txt 包含 1849 行, 92943 个词.\n",
      "./rumor_detection_data/rumor.txt 包含 1538 行, 78645 个词.\n",
      "过滤掉词频 <= 3的单词后，字典大小：6690\n"
     ]
    }
   ],
   "source": [
    "# 将文本中的标点符号过滤掉\n",
    "def filter_punc(sentence):\n",
    "    sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？?、~@#￥%……&*（）：:；“”】》《-【\\][]\", \"\",sentence.strip())\n",
    "    return sentence\n",
    "\n",
    "# 扫描所有的文本，分词、建立词典，分出是谣言还是非谣言，is_filter可以过滤是否筛选掉标点符号\n",
    "def Prepare_data(good_file, bad_file, is_filter = True, threshold=3):\n",
    "    all_words = [] #存储所有的单词\n",
    "    pos_sentences = [] #存储非谣言\n",
    "    neg_sentences = [] #存储谣言\n",
    "    with open(good_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                #过滤标点符号\n",
    "                line = filter_punc(line)\n",
    "                if not idx: # 只打印第一个例子看看\n",
    "                    print('分词前：', line)\n",
    "            #分词\n",
    "            words = jieba.lcut(line)\n",
    "            if not idx: # 只打印第一个例子看看\n",
    "                print('分词后：', words)\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                pos_sentences.append(words)\n",
    "    print('{0} 包含 {1} 行, {2} 个词.'.format(good_file, idx+1, len(all_words)))\n",
    "\n",
    "    count = len(all_words)\n",
    "    with open(bad_file, 'r', encoding='utf-8') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line.strip())\n",
    "            words = jieba.lcut(line)\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                neg_sentences.append(words)\n",
    "    print('{0} 包含 {1} 行, {2} 个词.'.format(bad_file, idx+1, len(all_words)-count))\n",
    "\n",
    "    #建立词典，只保留频次大于threshold的单词\n",
    "    vocab = {'<unk>': 0}\n",
    "    cnt = Counter(all_words)\n",
    "    for word, freq in cnt.items():\n",
    "        if freq > threshold:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    print('过滤掉词频 <= {}的单词后，字典大小：{}'.format(threshold, len(vocab)))\n",
    "    return pos_sentences, neg_sentences, vocab\n",
    "\n",
    "\n",
    "pos_sentences, neg_sentences, vocab = Prepare_data(good_file, bad_file, True, threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "sO3_QvSiZ0TO"
   },
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "Y3E85ZF-Z0TO"
   },
   "outputs": [],
   "source": [
    "# 获得句子的向量化表示\n",
    "def sentence2vec(word_ids, vocab_size):\n",
    "    vector = np.zeros(vocab_size)\n",
    "    for word_id in word_ids:\n",
    "        vector[word_id] += 1\n",
    "    return 1.0 * vector / len(word_ids)\n",
    "\n",
    "bow = [] #词袋\n",
    "labels = [] #标签\n",
    "sentences = [] #原始句子，调试用\n",
    "sentences_id = [] #原始句子对应的index列表\n",
    "\n",
    "# 处理非谣言\n",
    "for sentence in pos_sentences:\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        new_sentence.append(vocab[word] if word in vocab else vocab['<unk>'])\n",
    "\n",
    "    bow.append(sentence2vec(new_sentence, len(vocab)))\n",
    "    labels.append(0) #正标签为0\n",
    "    sentences.append(sentence)\n",
    "    sentences_id.append(new_sentence)\n",
    "\n",
    "# 处理谣言\n",
    "for sentence in neg_sentences:\n",
    "    new_sentence = []\n",
    "    for word in sentence:\n",
    "        new_sentence.append(vocab[word] if word in vocab else vocab['<unk>'])\n",
    "\n",
    "    bow.append(sentence2vec(new_sentence, len(vocab)))\n",
    "    labels.append(1) #负标签为1\n",
    "    sentences.append(sentence)\n",
    "    sentences_id.append(new_sentence)\n",
    "\n",
    "# 打乱所有的数据顺序，形成数据集\n",
    "# indices为所有数据下标的一个全排列\n",
    "indices = np.random.permutation(len(bow))\n",
    "\n",
    "#对整个数据集进行划分，分为：训练集、验证集和测试集，这里是2:1:1\n",
    "test_size = len(bow) // 4\n",
    "\n",
    "data = {\n",
    "    'bow': bow,# 词袋数据\n",
    "    'labels': labels,# 标签\n",
    "    'sentences_id': sentences_id,# 句子对应的下标列表\n",
    "    'sentences': sentences,# 句子\n",
    "    'vocab': vocab # 词典,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【K-Fold关键改动】——构造K-**Fold数据集划分**\n",
    "这里使用KFold类，首先，把测试数据集单独划分出来。然后对剩下的训练数据集做K-Fold划分：把数据集划分5个批次，每一次取其中一批为验证集，剩下4批为训练集。\n",
    "\n",
    "为了代码解耦，足够灵活，在实现上，构造了一个split列表splits，里面5个元素，每一个元素split都是一个字典，它代表了K-Fold过程中每一次训练的时候训练集、验证集、测试集的划分情况。\n",
    "\n",
    "在训练的时候，每一次训练都只需要把列表中的一个split字典配置传进去就行，能够和原来的代码无缝衔接，代码兼容性非常好。"
   ],
   "metadata": {
    "id": "SkX0xW-F9Ncb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# K-Fold交叉验证\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "splits = []\n",
    "test_split_index = indices[:test_size]\n",
    "train_split_index = indices[test_size:]\n",
    "\n",
    "for train_index, val_index in kf.split(train_split_index):\n",
    "    splits.append(\n",
    "        {\n",
    "          'train': train_split_index[train_index],\n",
    "          'vali': train_split_index[val_index],\n",
    "          'test': test_split_index\n",
    "        }\n",
    "    )\n"
   ],
   "metadata": {
    "id": "eQF6GUfbsK0-"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8zTsuU9Z0TO",
    "outputId": "7069b941-84b5-4ea9-fb89-d05bc799c41d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train 非谣言有1096条，谣言有936条\n",
      "vali 非谣言有282条，谣言有227条\n",
      "test 非谣言有471条，谣言有375条\n",
      "train 非谣言有1098条，谣言有935条\n",
      "vali 非谣言有280条，谣言有228条\n",
      "test 非谣言有471条，谣言有375条\n",
      "train 非谣言有1115条，谣言有918条\n",
      "vali 非谣言有263条，谣言有245条\n",
      "test 非谣言有471条，谣言有375条\n",
      "train 非谣言有1104条，谣言有929条\n",
      "vali 非谣言有274条，谣言有234条\n",
      "test 非谣言有471条，谣言有375条\n",
      "train 非谣言有1099条，谣言有934条\n",
      "vali 非谣言有279条，谣言有229条\n",
      "test 非谣言有471条，谣言有375条\n"
     ]
    }
   ],
   "source": [
    "# 查看一下划分情况\n",
    "for split in splits:\n",
    "  for key, indices in split.items():\n",
    "    count = [0, 0]\n",
    "    for idx in indices:\n",
    "        count[labels[idx]] += 1\n",
    "    print(key, '非谣言有{}条，谣言有{}条'.format(count[0], count[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "iO_z3s5SZ0TP"
   },
   "source": [
    "## 训练/测试函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "N9tbcVEHZ0TP"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    用于储存与计算平均值\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1, multiply=True):\n",
    "        self.val = val\n",
    "        if multiply:\n",
    "            self.sum += val * n\n",
    "        else:\n",
    "            self.sum += val\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def training(model, loader, crit, optim, device):\n",
    "    # 模型调成训练模式\n",
    "    model.train()\n",
    "    # 把模型移到指定设备\n",
    "    model.to(device)\n",
    "    # 用于记录损失和正确率\n",
    "    meter_loss, meter_acc = AverageMeter(), AverageMeter()\n",
    "\n",
    "    for data in loader:\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "        # 获取数据并将其移至指定设备中, cpu / gpu\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        # 将输入送入网络，获得输出\n",
    "        outputs = model(inputs)\n",
    "        # 计算损失\n",
    "        loss = crit(outputs, labels)\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "        # 更新网络参数\n",
    "        optim.step()\n",
    "\n",
    "        # 记录损失\n",
    "        num_sample = inputs.size(0)\n",
    "        meter_loss.update(loss.item(), num_sample)\n",
    "        # 记录预测正确率\n",
    "        preds = outputs.max(dim=1)[1] # 网络预测的类别结果\n",
    "        correct = (preds == labels).sum() # 计算预测的正确个数\n",
    "        meter_acc.update(correct.item(), num_sample, multiply=False)\n",
    "\n",
    "    # 返回训练集的平均损失和平均正确率\n",
    "    return meter_loss.avg, meter_acc.avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, crit, device):\n",
    "    # 模型调成评估模式\n",
    "    model.eval()\n",
    "    # 把模型移到指定设备\n",
    "    model.to(device)\n",
    "    # 用于记录损失和正确率\n",
    "    meter_loss, meter_acc = AverageMeter(), AverageMeter()\n",
    "    for data in loader:\n",
    "        # 获取数据并将其移至指定设备中, cpu / gpu\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        # 将输入送入网络，获得输出\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 计算并记录损失\n",
    "        loss = crit(outputs, labels)\n",
    "        num_sample = inputs.size(0)\n",
    "        meter_loss.update(loss.item(), num_sample)\n",
    "        # 记录预测正确率\n",
    "        preds = outputs.max(dim=1)[1] # 网络预测的类别结果\n",
    "        correct = (preds == labels).sum() # 计算预测的正确个数\n",
    "        meter_acc.update(correct.item(), num_sample, multiply=False)\n",
    "\n",
    "    return meter_loss.avg, meter_acc.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "E1vrpHg_Z0TQ"
   },
   "source": [
    "## 基于MLP的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "28TWHbwJZ0TQ"
   },
   "source": [
    "### 数据加载器定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "ROs-Bx1lZ0TQ"
   },
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        super().__init__()\n",
    "        self.make_dataset(data, split)\n",
    "\n",
    "    def make_dataset(self, data, split):\n",
    "        # Data是包含了整个数据集的数据\n",
    "        # 而我们只需要训练集/验证集/测试集的数据\n",
    "        # 我们按照划分基准split里面的下标来确定加载哪部分的数据\n",
    "        self.dataset = []\n",
    "        for idx in split:\n",
    "            item = [torch.FloatTensor(data['bow'][idx]),\n",
    "                    torch.LongTensor([data['labels'][idx]])]\n",
    "            self.dataset.append(item)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        # ix大于等于0，小于len(self.dataset)\n",
    "        return self.dataset[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 一共有多少数据\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def get_loader(data, split, batch_size=64, class_func=BaseDataset):\n",
    "    # split.keys() 包括 'train', 'vali', 'test'\n",
    "    # 所以此函数是为了拿到训练集，验证集和测试集的数据加载器\n",
    "    loader = []\n",
    "    for mode in split.keys():\n",
    "        # split[mode]指定了要取data的哪些数据\n",
    "        dataset = class_func(data, split[mode])\n",
    "        # Dataloader可帮助我们一次性取batch_size个样本出来\n",
    "        loader.append(\n",
    "            DataLoader(\n",
    "                dataset,\n",
    "                batch_size = batch_size,\n",
    "                shuffle = True if mode=='train' else False\n",
    "            )\n",
    "        )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmE4nKoVZ0TQ",
    "outputId": "c49988e1-1ba4-4dd9-ead7-ac922ec9f0f4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "词袋输入的形状： torch.Size([64, 6690])\n",
      "标签的形状： torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# 测试一下\n",
    "_, _, fake_loader = get_loader(data, splits[0], 64)\n",
    "x, y = iter(fake_loader).__next__()\n",
    "print('词袋输入的形状：', x.shape)\n",
    "print('标签的形状：', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "lhPAd95DZ0TR"
   },
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "BwT2tAvVZ0TR"
   },
   "outputs": [],
   "source": [
    "# 一个简单的前馈神经网络，三层，第一层线性层，加一个非线性ReLU，第二层线性层\n",
    "# 输入维度为词典的大小：每一段评论的词袋模型\n",
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_class=2, dropout=0):\n",
    "        super(Linear_Model,self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(vocab_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_size, num_class),\n",
    "                )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.net(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "rOLISMghZ0TR"
   },
   "source": [
    "### 【K-Fold关键改动】模型训练\n",
    "这里只需要把原来的训练代码外层套一个for循环，每一次循环的时候读取前面已经K-Fold划分好的split配置即可（它本质是提供了当次训练数据集的划分索引情况）。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 参数\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "batch_size = 32\n",
    "vocab_size = len(vocab)\n",
    "hidden_size = 10\n",
    "dropout = 0.0\n",
    "\n",
    "# 运行的设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# 进行K-Fold验证\n",
    "\n",
    "k_fold_val_acc = []\n",
    "k_fold_test_acc = []\n",
    "for val_index in range(5):\n",
    "  split = splits[val_index]\n",
    "  print('\\n交叉验证第%d / 5 轮训练\\n'% val_index)\n",
    "  # 数据加载器\n",
    "  train_loader, vali_loader, test_loader = get_loader(data, split, batch_size=batch_size, class_func=BaseDataset)\n",
    "  # 模型实例化\n",
    "  model = Linear_Model(vocab_size, hidden_size, dropout=dropout)\n",
    "  # 打印模型\n",
    "  # print(model)\n",
    "  # 损失函数 -- 交叉熵\n",
    "  crit = torch.nn.NLLLoss()\n",
    "  # 优化方法 -- Adam\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "  records = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    train_loss, train_acc = training(model, train_loader, crit, optimizer, device)\n",
    "    # 验证\n",
    "    vali_loss, vali_acc = evaluate(model, vali_loader, crit, device)\n",
    "    # 打印消息\n",
    "    print('第{}轮，训练集损失：{:.2f}, 训练集准确率：{:.2f}, 验证集损失：{:.2f}, 验证集准确率: {:.2f}'.format(\n",
    "        epoch, train_loss, train_acc, vali_loss, vali_acc))\n",
    "    # 储存信息以便可视化\n",
    "    records.append([train_loss, train_acc, vali_loss, vali_acc])\n",
    "    if epoch == num_epochs - 1:\n",
    "      k_fold_val_acc.append(vali_acc)\n",
    "  # 测试\n",
    "  _, test_acc = evaluate(model, test_loader, crit, device)\n",
    "  k_fold_test_acc.append(test_acc)\n",
    "  print('测试集正确率：', test_acc)\n",
    "\n",
    "print('K-fold交叉验证，验证集的平均准确率：%.4f, 测试集的平均准确率：%.4f' % (sum(k_fold_val_acc)/len(k_fold_val_acc),sum(k_fold_test_acc)/len(k_fold_test_acc)))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhjxA9QJ4n7q",
    "outputId": "f839cd2e-56fd-4afd-9aca-0274c2500e96"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "交叉验证第0 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.66, 训练集准确率：0.59, 验证集损失：0.61, 验证集准确率: 0.71\n",
      "第1轮，训练集损失：0.52, 训练集准确率：0.81, 验证集损失：0.46, 验证集准确率: 0.87\n",
      "第2轮，训练集损失：0.37, 训练集准确率：0.89, 验证集损失：0.38, 验证集准确率: 0.88\n",
      "第3轮，训练集损失：0.27, 训练集准确率：0.93, 验证集损失：0.34, 验证集准确率: 0.88\n",
      "第4轮，训练集损失：0.20, 训练集准确率：0.96, 验证集损失：0.31, 验证集准确率: 0.90\n",
      "第5轮，训练集损失：0.15, 训练集准确率：0.97, 验证集损失：0.30, 验证集准确率: 0.89\n",
      "第6轮，训练集损失：0.12, 训练集准确率：0.98, 验证集损失：0.28, 验证集准确率: 0.89\n",
      "第7轮，训练集损失：0.09, 训练集准确率：0.99, 验证集损失：0.28, 验证集准确率: 0.88\n",
      "第8轮，训练集损失：0.07, 训练集准确率：0.99, 验证集损失：0.28, 验证集准确率: 0.88\n",
      "第9轮，训练集损失：0.06, 训练集准确率：0.99, 验证集损失：0.28, 验证集准确率: 0.89\n",
      "测试集正确率： 0.8829787234042553\n",
      "\n",
      "交叉验证第1 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.66, 训练集准确率：0.62, 验证集损失：0.60, 验证集准确率: 0.76\n",
      "第1轮，训练集损失：0.49, 训练集准确率：0.84, 验证集损失：0.45, 验证集准确率: 0.84\n",
      "第2轮，训练集损失：0.34, 训练集准确率：0.90, 验证集损失：0.38, 验证集准确率: 0.86\n",
      "第3轮，训练集损失：0.25, 训练集准确率：0.93, 验证集损失：0.34, 验证集准确率: 0.87\n",
      "第4轮，训练集损失：0.19, 训练集准确率：0.96, 验证集损失：0.32, 验证集准确率: 0.87\n",
      "第5轮，训练集损失：0.14, 训练集准确率：0.97, 验证集损失：0.30, 验证集准确率: 0.87\n",
      "第6轮，训练集损失：0.11, 训练集准确率：0.98, 验证集损失：0.29, 验证集准确率: 0.88\n",
      "第7轮，训练集损失：0.08, 训练集准确率：0.99, 验证集损失：0.29, 验证集准确率: 0.88\n",
      "第8轮，训练集损失：0.06, 训练集准确率：0.99, 验证集损失：0.29, 验证集准确率: 0.88\n",
      "第9轮，训练集损失：0.05, 训练集准确率：1.00, 验证集损失：0.29, 验证集准确率: 0.89\n",
      "测试集正确率： 0.8806146572104019\n",
      "\n",
      "交叉验证第2 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.67, 训练集准确率：0.54, 验证集损失：0.63, 验证集准确率: 0.65\n",
      "第1轮，训练集损失：0.57, 训练集准确率：0.77, 验证集损失：0.50, 验证集准确率: 0.81\n",
      "第2轮，训练集损失：0.43, 训练集准确率：0.86, 验证集损失：0.40, 验证集准确率: 0.85\n",
      "第3轮，训练集损失：0.32, 训练集准确率：0.91, 验证集损失：0.34, 验证集准确率: 0.86\n",
      "第4轮，训练集损失：0.25, 训练集准确率：0.93, 验证集损失：0.30, 验证集准确率: 0.88\n",
      "第5轮，训练集损失：0.20, 训练集准确率：0.96, 验证集损失：0.28, 验证集准确率: 0.90\n",
      "第6轮，训练集损失：0.16, 训练集准确率：0.97, 验证集损失：0.27, 验证集准确率: 0.90\n",
      "第7轮，训练集损失：0.12, 训练集准确率：0.98, 验证集损失：0.26, 验证集准确率: 0.91\n",
      "第8轮，训练集损失：0.10, 训练集准确率：0.99, 验证集损失：0.25, 验证集准确率: 0.91\n",
      "第9轮，训练集损失：0.08, 训练集准确率：0.99, 验证集损失：0.24, 验证集准确率: 0.91\n",
      "测试集正确率： 0.8888888888888888\n",
      "\n",
      "交叉验证第3 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.67, 训练集准确率：0.58, 验证集损失：0.63, 验证集准确率: 0.61\n",
      "第1轮，训练集损失：0.54, 训练集准确率：0.79, 验证集损失：0.49, 验证集准确率: 0.83\n",
      "第2轮，训练集损失：0.38, 训练集准确率：0.89, 验证集损失：0.41, 验证集准确率: 0.82\n",
      "第3轮，训练集损失：0.28, 训练集准确率：0.92, 验证集损失：0.37, 验证集准确率: 0.83\n",
      "第4轮，训练集损失：0.21, 训练集准确率：0.95, 验证集损失：0.35, 验证集准确率: 0.85\n",
      "第5轮，训练集损失：0.16, 训练集准确率：0.96, 验证集损失：0.33, 验证集准确率: 0.85\n",
      "第6轮，训练集损失：0.12, 训练集准确率：0.98, 验证集损失：0.33, 验证集准确率: 0.85\n",
      "第7轮，训练集损失：0.10, 训练集准确率：0.99, 验证集损失：0.32, 验证集准确率: 0.85\n",
      "第8轮，训练集损失：0.08, 训练集准确率：0.99, 验证集损失：0.32, 验证集准确率: 0.85\n",
      "第9轮，训练集损失：0.06, 训练集准确率：0.99, 验证集损失：0.33, 验证集准确率: 0.85\n",
      "测试集正确率： 0.8782505910165485\n",
      "\n",
      "交叉验证第4 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.68, 训练集准确率：0.54, 验证集损失：0.66, 验证集准确率: 0.70\n",
      "第1轮，训练集损失：0.61, 训练集准确率：0.80, 验证集损失：0.56, 验证集准确率: 0.81\n",
      "第2轮，训练集损失：0.47, 训练集准确率：0.87, 验证集损失：0.46, 验证集准确率: 0.84\n",
      "第3轮，训练集损失：0.36, 训练集准确率：0.90, 验证集损失：0.42, 验证集准确率: 0.85\n",
      "第4轮，训练集损失：0.29, 训练集准确率：0.91, 验证集损失：0.39, 验证集准确率: 0.86\n",
      "第5轮，训练集损失：0.22, 训练集准确率：0.95, 验证集损失：0.36, 验证集准确率: 0.86\n",
      "第6轮，训练集损失：0.16, 训练集准确率：0.97, 验证集损失：0.34, 验证集准确率: 0.87\n",
      "第7轮，训练集损失：0.12, 训练集准确率：0.98, 验证集损失：0.33, 验证集准确率: 0.87\n",
      "第8轮，训练集损失：0.09, 训练集准确率：0.99, 验证集损失：0.33, 验证集准确率: 0.87\n",
      "第9轮，训练集损失：0.07, 训练集准确率：0.99, 验证集损失：0.33, 验证集准确率: 0.88\n",
      "测试集正确率： 0.8900709219858156\n",
      "K-fold交叉验证，验证集的平均准确率：0.8835, 测试集的平均准确率：0.8842\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "xRlIJJ81Z0TS"
   },
   "source": [
    "## 基于Transformer的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "5_B6gznpZ0TS"
   },
   "source": [
    "### 数据加载器定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "Ph5X-v5vZ0TS"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        super().__init__()\n",
    "        self.vocab = data['vocab']\n",
    "        self.pad_index = len(self.vocab.keys()) if '<pad>' not in self.vocab.keys() else self.vocab['<pad>']\n",
    "        self.max_len = data.get('max_len', 30)\n",
    "        self.make_dataset(data, split)\n",
    "\n",
    "    def make_dataset(self, data, split):\n",
    "        # Data是包含了整个数据集的数据\n",
    "        # 而我们只需要训练集/验证集/测试集的数据\n",
    "        # 我们按照划分基准split里面的下标来确定加载哪部分的数据\n",
    "        self.dataset = []\n",
    "        for idx in split:\n",
    "            this_sentence_id = data['sentences_id'][idx]\n",
    "            item = [\n",
    "                torch.LongTensor(self.pad_data(this_sentence_id)),\n",
    "                torch.LongTensor([data['labels'][idx]])\n",
    "            ]\n",
    "            self.dataset.append(item)\n",
    "\n",
    "    def pad_data(self, seq):\n",
    "        # 让序列长度最长只有max_len，不足就补pad，超过就截断\n",
    "        if len(seq) < self.max_len:\n",
    "            seq += [self.pad_index] * (self.max_len - len(seq))\n",
    "\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        return seq\n",
    "\n",
    "    def get_pad_index(self):\n",
    "        return self.pad_index\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        # ix大于等于0，小于len(self.dataset)\n",
    "        return self.dataset[ix]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 一共有多少数据\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "dsIFhVfNZ0TT"
   },
   "source": [
    "### Transformer基础组件实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "01NM_ByCZ0TT"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_head_size = hidden_size // num_heads\n",
    "        self.all_head_size = hidden_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape) # [bsz, seq_len, n_head, head_size]\n",
    "        return x.permute(0, 2, 1, 3) # [bsz, n_head, seq_len, head_size]\n",
    "\n",
    "    def forward(self, q, k, v, attention_mask=None):\n",
    "        query = self.transpose_for_scores(self.query(q)) # [bsz, n_head, lq, head_size]\n",
    "        key = self.transpose_for_scores(self.key(k))     # [bsz, n_head, lk, head_size]\n",
    "        value = self.transpose_for_scores(self.value(v)) # [bsz, n_head, lv, head_size]\n",
    "\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) # [bsz, n_head, lq, lk]\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.dim() == 2:\n",
    "                attention_mask = attention_mask[:, None, None, :] # [bsz, 1, 1, lk]\n",
    "            if attention_mask.dim() == 3:\n",
    "                attention_mask = attention_mask[:, None, :, :] # [bsz, 1, lq, lk]\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask, -1e9)\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs) # [bsz, n_head, lq, lk]\n",
    "\n",
    "        context = torch.matmul(attention_probs, value) # [bsz, n_head, lq, head_size]\n",
    "        context = context.permute(0, 2, 1, 3).contiguous() # [bsz, lq, n_head, head_size]\n",
    "\n",
    "        new_context_shape = context.size()[:-2] + (self.all_head_size,)\n",
    "        context = context.view(*new_context_shape) # [bsz, lq, dim_hidden]\n",
    "\n",
    "        return context, attention_probs\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, attn_dropout, dropout):\n",
    "        super().__init__()\n",
    "        self.SDPA = ScaledDotProductAttention(hidden_size, num_heads, attn_dropout)\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        q, k, v = hidden_states, hidden_states, hidden_states\n",
    "        context, attention_probs = self.SDPA(q, k, v, attention_mask)\n",
    "        context = self.dense(context)\n",
    "        context = self.dropout(context)\n",
    "\n",
    "        hidden_states = self.LayerNorm(hidden_states + context)\n",
    "        return hidden_states, attention_probs\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, dropout=.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.LN = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.LN(x + self.net(x))\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, intermediate_size, attn_dropout=.0, dropout=.0):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(hidden_size, num_heads, attn_dropout, dropout)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, intermediate_size, dropout)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        hidden_states, _ = self.mha(hidden_states, attention_mask)\n",
    "        hidden_states = self.ffn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_heads, intermediate_size, attn_dropout=.0, dropout=.0):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(hidden_size, num_heads, intermediate_size, attn_dropout, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "BrpRTs6LZ0TU"
   },
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "id": "pAjowiNHZ0TU"
   },
   "outputs": [],
   "source": [
    "class TFModel(nn.Module):\n",
    "    def __init__(self,\n",
    "            vocab_size,\n",
    "            hidden_size,\n",
    "            max_len,\n",
    "            pad_index,\n",
    "            num_class=2,\n",
    "            num_heads=4,\n",
    "            num_layers=1,\n",
    "            dropout=0.0,\n",
    "            attn_dropout=0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.word_embs = nn.Embedding(vocab_size + 1, hidden_size, padding_idx=pad_index)\n",
    "        self.position_embs = nn.Embedding(max_len+1, hidden_size)\n",
    "\n",
    "        self.net = TransformerEncoder(\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            4 * hidden_size,\n",
    "            attn_dropout,\n",
    "            dropout\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.prj = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "        self.cls_index = vocab_size\n",
    "        self.pad_index = pad_index\n",
    "\n",
    "        nn.init.normal_(self.word_embs.weight, std=.02)\n",
    "        nn.init.normal_(self.position_embs.weight, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        batch_size, device = input_ids.size(0), input_ids.device\n",
    "        cls_tokens = torch.zeros((batch_size, 1)).to(input_ids.device) + self.cls_index\n",
    "        cls_tokens = cls_tokens.long()\n",
    "        input_ids = torch.cat((cls_tokens, input_ids), dim=1)\n",
    "\n",
    "        embs = self.word_embs(input_ids)\n",
    "\n",
    "        seq_len = embs.size(1)\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids[None, :]\n",
    "        position_embs = self.position_embs(position_ids)\n",
    "\n",
    "        embs = embs + position_embs\n",
    "        embs = self.dropout(embs)\n",
    "\n",
    "        attention_mask = (input_ids == self.pad_index) # (batch_size, seq_len)\n",
    "        hidden_states = self.net(embs, attention_mask)\n",
    "\n",
    "        cls_hidden_state = hidden_states[:, 0, :]\n",
    "        cls_hidden_state = self.dropout(cls_hidden_state)\n",
    "\n",
    "        output = self.prj(cls_hidden_state)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        return output\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "id": "hAC8ujU2Z0TU"
   },
   "source": [
    "### 【K-Fold关键改动】transformer K-Fold训练\n",
    "和前面的MLP模型一样，只需要把原来的训练代码外层包一层for循环，在每一个训练的时候，加载对应的split配置（它提供了当次训练数据集的划分情况的索引）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1st0W5jZ0TU",
    "outputId": "d86a2de4-6041-4db6-a54f-c11c4bc3af28"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "交叉验证第0 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.69, 训练集准确率：0.55, 验证集损失：0.68, 验证集准确率: 0.53\n",
      "第1轮，训练集损失：0.55, 训练集准确率：0.77, 验证集损失：0.38, 验证集准确率: 0.86\n",
      "第2轮，训练集损失：0.25, 训练集准确率：0.92, 验证集损失：0.35, 验证集准确率: 0.88\n",
      "第3轮，训练集损失：0.14, 训练集准确率：0.96, 验证集损失：0.40, 验证集准确率: 0.88\n",
      "第4轮，训练集损失：0.09, 训练集准确率：0.98, 验证集损失：0.42, 验证集准确率: 0.88\n",
      "第5轮，训练集损失：0.06, 训练集准确率：0.99, 验证集损失：0.46, 验证集准确率: 0.87\n",
      "第6轮，训练集损失：0.04, 训练集准确率：0.99, 验证集损失：0.44, 验证集准确率: 0.89\n",
      "第7轮，训练集损失：0.03, 训练集准确率：1.00, 验证集损失：0.54, 验证集准确率: 0.88\n",
      "第8轮，训练集损失：0.02, 训练集准确率：1.00, 验证集损失：0.58, 验证集准确率: 0.88\n",
      "第9轮，训练集损失：0.02, 训练集准确率：1.00, 验证集损失：0.68, 验证集准确率: 0.88\n",
      "测试集正确率： 0.8605200945626478\n",
      "测试集正确率： 0.8605200945626478\n",
      "\n",
      "交叉验证第1 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.69, 训练集准确率：0.54, 验证集损失：0.69, 验证集准确率: 0.55\n",
      "第1轮，训练集损失：0.62, 训练集准确率：0.68, 验证集损失：0.42, 验证集准确率: 0.82\n",
      "第2轮，训练集损失：0.31, 训练集准确率：0.90, 验证集损失：0.33, 验证集准确率: 0.88\n",
      "第3轮，训练集损失：0.16, 训练集准确率：0.95, 验证集损失：0.40, 验证集准确率: 0.87\n",
      "第4轮，训练集损失：0.10, 训练集准确率：0.97, 验证集损失：0.39, 验证集准确率: 0.89\n",
      "第5轮，训练集损失：0.06, 训练集准确率：0.99, 验证集损失：0.49, 验证集准确率: 0.88\n",
      "第6轮，训练集损失：0.03, 训练集准确率：0.99, 验证集损失：0.47, 验证集准确率: 0.88\n",
      "第7轮，训练集损失：0.02, 训练集准确率：1.00, 验证集损失：0.56, 验证集准确率: 0.88\n",
      "第8轮，训练集损失：0.01, 训练集准确率：1.00, 验证集损失：0.67, 验证集准确率: 0.86\n",
      "第9轮，训练集损失：0.01, 训练集准确率：1.00, 验证集损失：0.68, 验证集准确率: 0.86\n",
      "测试集正确率： 0.8735224586288416\n",
      "测试集正确率： 0.8735224586288416\n",
      "\n",
      "交叉验证第2 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.69, 训练集准确率：0.52, 验证集损失：0.68, 验证集准确率: 0.54\n",
      "第1轮，训练集损失：0.53, 训练集准确率：0.78, 验证集损失：0.38, 验证集准确率: 0.86\n",
      "第2轮，训练集损失：0.26, 训练集准确率：0.92, 验证集损失：0.33, 验证集准确率: 0.87\n",
      "第3轮，训练集损失：0.14, 训练集准确率：0.96, 验证集损失：0.33, 验证集准确率: 0.90\n",
      "第4轮，训练集损失：0.06, 训练集准确率：0.98, 验证集损失：0.41, 验证集准确率: 0.88\n",
      "第5轮，训练集损失：0.05, 训练集准确率：0.99, 验证集损失：0.54, 验证集准确率: 0.87\n",
      "第6轮，训练集损失：0.04, 训练集准确率：0.99, 验证集损失：0.47, 验证集准确率: 0.89\n",
      "第7轮，训练集损失：0.02, 训练集准确率：1.00, 验证集损失：0.49, 验证集准确率: 0.90\n",
      "第8轮，训练集损失：0.01, 训练集准确率：1.00, 验证集损失：0.58, 验证集准确率: 0.89\n",
      "第9轮，训练集损失：0.02, 训练集准确率：1.00, 验证集损失：0.58, 验证集准确率: 0.89\n",
      "测试集正确率： 0.8699763593380615\n",
      "测试集正确率： 0.8699763593380615\n",
      "\n",
      "交叉验证第3 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.69, 训练集准确率：0.54, 验证集损失：0.69, 验证集准确率: 0.53\n",
      "第1轮，训练集损失：0.59, 训练集准确率：0.72, 验证集损失：0.42, 验证集准确率: 0.83\n",
      "第2轮，训练集损失：0.30, 训练集准确率：0.90, 验证集损失：0.43, 验证集准确率: 0.84\n",
      "第3轮，训练集损失：0.17, 训练集准确率：0.95, 验证集损失：0.40, 验证集准确率: 0.86\n",
      "第4轮，训练集损失：0.09, 训练集准确率：0.98, 验证集损失：0.51, 验证集准确率: 0.87\n",
      "第5轮，训练集损失：0.05, 训练集准确率：0.99, 验证集损失：0.57, 验证集准确率: 0.86\n",
      "第6轮，训练集损失：0.05, 训练集准确率：0.99, 验证集损失：0.53, 验证集准确率: 0.87\n",
      "第7轮，训练集损失：0.03, 训练集准确率：0.99, 验证集损失：0.53, 验证集准确率: 0.87\n",
      "第8轮，训练集损失：0.03, 训练集准确率：1.00, 验证集损失：0.63, 验证集准确率: 0.86\n",
      "第9轮，训练集损失：0.02, 训练集准确率：1.00, 验证集损失：0.69, 验证集准确率: 0.86\n",
      "测试集正确率： 0.8711583924349882\n",
      "测试集正确率： 0.8711583924349882\n",
      "\n",
      "交叉验证第4 / 5 轮训练\n",
      "\n",
      "第0轮，训练集损失：0.69, 训练集准确率：0.53, 验证集损失：0.67, 验证集准确率: 0.72\n",
      "第1轮，训练集损失：0.55, 训练集准确率：0.75, 验证集损失：0.42, 验证集准确率: 0.83\n",
      "第2轮，训练集损失：0.26, 训练集准确率：0.91, 验证集损失：0.37, 验证集准确率: 0.85\n",
      "第3轮，训练集损失：0.12, 训练集准确率：0.97, 验证集损失：0.44, 验证集准确率: 0.88\n",
      "第4轮，训练集损失：0.06, 训练集准确率：0.99, 验证集损失：0.52, 验证集准确率: 0.87\n",
      "第5轮，训练集损失：0.04, 训练集准确率：0.99, 验证集损失：0.55, 验证集准确率: 0.89\n",
      "第6轮，训练集损失：0.03, 训练集准确率：1.00, 验证集损失：0.66, 验证集准确率: 0.89\n",
      "第7轮，训练集损失：0.03, 训练集准确率：0.99, 验证集损失：0.63, 验证集准确率: 0.88\n",
      "第8轮，训练集损失：0.03, 训练集准确率：0.99, 验证集损失：0.68, 验证集准确率: 0.86\n",
      "第9轮，训练集损失：0.03, 训练集准确率：1.00, 验证集损失：0.57, 验证集准确率: 0.88\n",
      "测试集正确率： 0.8652482269503546\n",
      "测试集正确率： 0.8652482269503546\n",
      "K-fold交叉验证，验证集的平均准确率：0.8768, 测试集的平均准确率：0.8681\n"
     ]
    }
   ],
   "source": [
    "# 参数\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "batch_size = 128\n",
    "vocab_size = len(vocab)+1\n",
    "hidden_size = 16\n",
    "num_heads = 4\n",
    "num_layers = 1\n",
    "attn_dropout = 0.1\n",
    "dropout = 0.3\n",
    "\n",
    "data['max_len'] = 100\n",
    "\n",
    "# 运行的设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# 进行K-Fold验证\n",
    "\n",
    "k_fold_val_acc = []\n",
    "k_fold_test_acc = []\n",
    "for val_index in range(5):\n",
    "  split = splits[val_index]\n",
    "  print('\\n交叉验证第%d / 5 轮训练\\n'% val_index)\n",
    "  # 数据加载器\n",
    "  train_loader, vali_loader, test_loader = get_loader(data, split, batch_size=batch_size, class_func=MyDataset)\n",
    "  # 模型实例化\n",
    "  model = TFModel(\n",
    "    vocab_size,\n",
    "    hidden_size,\n",
    "    data['max_len'],\n",
    "    pad_index=train_loader.dataset.get_pad_index(),\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    attn_dropout=attn_dropout,\n",
    "    dropout=dropout\n",
    "  )\n",
    "  # 打印模型\n",
    "  #print(model)\n",
    "  # 损失函数 -- 交叉熵\n",
    "  crit = torch.nn.NLLLoss()\n",
    "  # 优化方法\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  records = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    train_loss, train_acc = training(model, train_loader, crit, optimizer, device)\n",
    "    # 验证\n",
    "    vali_loss, vali_acc = evaluate(model, vali_loader, crit, device)\n",
    "    # 打印消息\n",
    "    print('第{}轮，训练集损失：{:.2f}, 训练集准确率：{:.2f}, 验证集损失：{:.2f}, 验证集准确率: {:.2f}'.format(\n",
    "        epoch, train_loss, train_acc, vali_loss, vali_acc))\n",
    "    # 储存信息以便可视化\n",
    "    records.append([train_loss, train_acc, vali_loss, vali_acc])\n",
    "    if epoch == num_epochs - 1:\n",
    "      k_fold_val_acc.append(vali_acc)\n",
    "  # 测试\n",
    "  _, test_acc = evaluate(model, test_loader, crit, device)\n",
    "  print('测试集正确率：', test_acc)\n",
    "  k_fold_test_acc.append(test_acc)\n",
    "  print('测试集正确率：', test_acc)\n",
    "\n",
    "print('K-fold交叉验证，验证集的平均准确率：%.4f, 测试集的平均准确率：%.4f' % (sum(k_fold_val_acc)/len(k_fold_val_acc),sum(k_fold_test_acc)/len(k_fold_test_acc)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "TDXSLg0iCiFa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 任务二.\n",
    "**改进**，设法提升谣言检测的准确率\n",
    "\n",
    "\n",
    "**结论：**\n",
    "经过K-Fold验证，改进后的方法的谣言检测准确率**平均是86.81%**"
   ],
   "metadata": {
    "id": "HmMk24XNDwww"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集加载\n",
    "原先的词袋模型采用one-hot的方法编码词向量，丢失了很多词语之间的关联性信息，所以考虑换成transformer的token化方法对词向量进行编码。"
   ],
   "metadata": {
    "id": "D5jCwOTMIj5l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\" # 模型加载需要\n",
    "\n",
    "# 数据来源文件夹 -- 内含多个json文件\n",
    "non_rumor = './Chinese_Rumor_Dataset/CED_Dataset/non-rumor-repost'\n",
    "rumor = './Chinese_Rumor_Dataset/CED_Dataset/rumor-repost'\n",
    "original = './Chinese_Rumor_Dataset/CED_Dataset/original-microblog'\n",
    "\n",
    "non_rumor_data = []\n",
    "rumor_data = []\n",
    "\n",
    "# 遍历文件夹，读取文本数据\n",
    "print('开始读取数据')\n",
    "for file in tqdm(os.listdir(original)):\n",
    "    try:\n",
    "        data = json.load(open(os.path.join(original, file), 'rb'))['text']\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    is_rumor = (file in os.listdir(rumor))\n",
    "    if is_rumor:\n",
    "        rumor_data.append(data)\n",
    "    else:\n",
    "        non_rumor_data.append(data)\n",
    "\n",
    "print('结束, 有{}条谣言, 有{}条非谣言!'.format(len(rumor_data), len(non_rumor_data)))\n",
    "print(non_rumor_data[-2:])\n",
    "print('-'*20)\n",
    "print(rumor_data[-2:])\n",
    "\n",
    "labels = [0]*len(non_rumor_data)\n",
    "labels.extend([1]*len(rumor_data))\n",
    "data = []\n",
    "data.extend(non_rumor_data)\n",
    "data.extend(rumor_data)\n",
    "\n",
    "print('datalength:%d,labels length:%d'%(len(data),len(labels)))\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9jwgGsCnYQu",
    "outputId": "97ceff64-096e-4045-acd7-18ba152c84d2"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "开始读取数据\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3389/3389 [00:06<00:00, 498.02it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "结束, 有1538条谣言, 有1849条非谣言!\n",
      "['今晚研究百合网，婚恋网站，因为明天做会议主持要对话百合网老大。第一次注册，看到好多姑娘的照片，但是系统给我推荐的怎么全是30岁以上的姑娘呢？在百合上看到一个有意思的统计分析，见附图~，女人应该在25岁之前把自己嫁出去哦~', '卡索拉上季在西甲创造84次射门机会，仅次于厄齐尔、梅西和纳瓦斯，他决心加盟枪手前还从皮雷和小法那得到很多鼓励。阿森纳队近年来罕见地一口气引进波多尔斯基、吉鲁和卡索拉三位国脚级新援。高高在上的范大将军，曼城、尤文图斯都打退堂鼓了，难道你真的，真的下定决心要走出“枪门”？']\n",
      "--------------------\n",
      "['李双江之子李天一涉嫌强奸罪于7日被批捕。李的76人律师团领队、法律大学副校长张爱国教授对媒体表示，李天一因第一个与被害女子发生性关系，不构成轮奸罪，只是以判罚较轻的强奸罪批捕，这是律师团所有成员共同努力的结果。http://t.cn/zYmXiH2', '「求辟谣：常务副市长陈尸护城河」原铁岭市常务副市长清华才子袁卫亮尸沉沈阳护城河！当地有关 部门及个别领导正在全力以赴不惜代价不择手段组织在各大论坛贴吧博客删帖，仅此一 贴，即创昨日中国删帖经济历史最高，个别管理单笔收入超过10万元，现在关于袁卫亮死讯99%以上都打不开，令死因扑朔迷离。 \\u200b']\n",
      "datalength:3387,labels length:3387\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ],
   "metadata": {
    "id": "GNRETFpHquob"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class RumorDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "id": "nuRELkwFqxhi"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"clue/roberta_chinese_base\")\n",
    "model = BertModel.from_pretrained(\"clue/roberta_chinese_base\")\n",
    "\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta_chinese_base')\n",
    "# model = RobertaForSequenceClassification.from_pretrained('roberta-base-chinese', num_labels=2)\n",
    "\n",
    "\n",
    "max_len = 512  # 可以根据实际情况调整最大序列长度\n",
    "\n",
    "\n",
    "# 创建数据集实例\n",
    "dataset = RumorDataset(data, labels, tokenizer, max_len)\n",
    "\n",
    "# 创建数据加载器，并设置shuffle=True以打散数据集\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZ_bt5k0rPQt",
    "outputId": "000138a6-d614-4703-836b-37af3fbd9c4f"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    print('epoch start')\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    iter = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('iter %d loss = %.2f'%(iter, loss / len(batch)))\n",
    "        iter = iter + 1\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy"
   ],
   "metadata": {
    "id": "uUzZ9QK8rNVL"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    accuracy = evaluate(model, train_loader, device)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EsDhau8PrVmc",
    "outputId": "5ca4b077-e25c-4359-c993-7319aab70d6a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch start\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "nDzRqN5nrXOI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "EyfxUI1IrW-C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        super().__init__()\n",
    "        self.vocab = data['vocab']\n",
    "        self.pad_index = len(self.vocab.keys()) if '<pad>' not in self.vocab.keys() else self.vocab['<pad>']\n",
    "        self.max_len = data.get('max_len', 30)\n",
    "        self.make_dataset(data, split)\n",
    "\n",
    "    def make_dataset(self, data, split):\n",
    "        self.dataset = []\n",
    "        for idx in split:\n",
    "            this_sentence_id = data['sentences_id'][idx]\n",
    "            item = [\n",
    "                torch.LongTensor(self.pad_data(this_sentence_id)),\n",
    "                torch.LongTensor([data['labels'][idx]]),\n",
    "                # 保存原始序列长度，用于生成attention_mask\n",
    "                torch.LongTensor([len(this_sentence_id)])\n",
    "            ]\n",
    "            self.dataset.append(item)\n",
    "\n",
    "    def pad_data(self, seq):\n",
    "        if len(seq) < self.max_len:\n",
    "            seq += [self.pad_index] * (self.max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        return seq\n",
    "\n",
    "    def get_pad_index(self):\n",
    "        return self.pad_index\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        item = self.dataset[ix]\n",
    "        input_ids = item[0]\n",
    "        label = item[1]\n",
    "        orig_len = item[2].item()  # 获取原始序列长度\n",
    "        attention_mask = torch.zeros(self.max_len, dtype=torch.long)\n",
    "        attention_mask[:orig_len] = 1  # 将原始序列部分设置为1\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ],
   "metadata": {
    "id": "dau0LpoAIi-k"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义训练和测试函数"
   ],
   "metadata": {
    "id": "XKskWRpOJ3Qg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    用于储存与计算平均值\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1, multiply=True):\n",
    "        self.val = val\n",
    "        if multiply:\n",
    "            self.sum += val * n\n",
    "        else:\n",
    "            self.sum += val\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def training(model, loader, crit, optim, device):\n",
    "    # 模型调成训练模式\n",
    "    model.train()\n",
    "    # 把模型移到指定设备\n",
    "    model.to(device)\n",
    "    # 用于记录损失和正确率\n",
    "    meter_loss, meter_acc = AverageMeter(), AverageMeter()\n",
    "    meter_loss.reset()\n",
    "    meter_acc.reset()\n",
    "    for data in loader:\n",
    "        # 清空梯度\n",
    "        optim.zero_grad()\n",
    "        # 获取数据并将其移至指定设备中, cpu / gpu\n",
    "        inputs, mask, labels = data\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        all_inputs = {\n",
    "            'input_ids': inputs,\n",
    "            'attention_mask': mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "        # 将输入送入网络，获得输出\n",
    "        outputs = model(**all_inputs).logits\n",
    "\n",
    "        # 计算损失\n",
    "        loss = crit(outputs, labels)\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "        # 更新网络参数\n",
    "        optim.step()\n",
    "\n",
    "        # 记录损失\n",
    "        num_sample = inputs.size(0)\n",
    "        meter_loss.update(loss.item(), num_sample, multiply=False)\n",
    "        # 记录预测正确率\n",
    "        preds = outputs.max(dim=1)[1] # 网络预测的类别结果\n",
    "        correct = (preds == labels).sum() # 计算预测的正确个数\n",
    "        meter_acc.update(correct.item(), num_sample, multiply=False)\n",
    "\n",
    "    # 返回训练集的平均损失和平均正确率\n",
    "    return meter_loss.avg, meter_acc.avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, crit, device):\n",
    "    # 模型调成评估模式\n",
    "    model.eval()\n",
    "    # 把模型移到指定设备\n",
    "    model.to(device)\n",
    "    # 用于记录损失和正确率\n",
    "    meter_loss, meter_acc = AverageMeter(), AverageMeter()\n",
    "    meter_loss.reset()\n",
    "    meter_acc.reset()\n",
    "    for data in loader:\n",
    "        # 获取数据并将其移至指定设备中, cpu / gpu\n",
    "        inputs, mask, labels = data\n",
    "        inputs, mask, labels = inputs.to(device), mask.to(device), labels.to(device)\n",
    "        labels = labels.view(-1)\n",
    "        all_inputs = {\n",
    "            'input_ids': inputs,\n",
    "            'attention_mask': mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "        # 将输入送入网络，获得输出\n",
    "        outputs = model(**all_inputs).logits\n",
    "\n",
    "        # 计算并记录损失\n",
    "        loss = crit(outputs, labels)\n",
    "        print(outputs, labels)\n",
    "        num_sample = inputs.size(0)\n",
    "        meter_loss.update(loss.item(), num_sample,multiply=False)\n",
    "        # 记录预测正确率\n",
    "        preds = outputs.max(dim=1)[1] # 网络预测的类别结果\n",
    "        correct = (preds == labels).sum() # 计算预测的正确个数\n",
    "        meter_acc.update(correct.item(), num_sample, multiply=False)\n",
    "\n",
    "    return meter_loss.avg, meter_acc.avg"
   ],
   "metadata": {
    "id": "w1GRT1e4J6_u"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型训练"
   ],
   "metadata": {
    "id": "7vhhp0PPJHZb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 2e-6\n",
    "# 运行的设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# 进行K-Fold验证\n",
    "\n",
    "k_fold_val_acc = []\n",
    "k_fold_test_acc = []\n",
    "for val_index in range(5):\n",
    "  split = splits[val_index]\n",
    "  print('\\n交叉验证第%d / 5 轮训练\\n'% val_index)\n",
    "  # 数据加载器\n",
    "  train_loader, vali_loader, test_loader = get_loader(data, split, batch_size=8, class_func=MyDataset)\n",
    "  # 加载预训练的BERT模型和分词器\n",
    "  model_name = 'bert-base-uncased'\n",
    "  tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "  model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "  # 打印模型\n",
    "  #print(model)\n",
    "  # 损失函数 -- 交叉熵\n",
    "  crit = torch.nn.NLLLoss()\n",
    "  # 优化方法\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  records = []\n",
    "  for epoch in range(num_epochs):\n",
    "    # 训练\n",
    "    train_loss, train_acc = training(model, train_loader, crit, optimizer, device)\n",
    "    # 验证\n",
    "    vali_loss, vali_acc = evaluate(model, vali_loader, crit, device)\n",
    "    # 打印消息\n",
    "    print('第{}轮，训练集损失：{:.2f}, 训练集准确率：{:.2f}, 验证集损失：{:.2f}, 验证集准确率: {:.2f}'.format(\n",
    "        epoch, train_loss, train_acc, vali_loss, vali_acc))\n",
    "    # 储存信息以便可视化\n",
    "    records.append([train_loss, train_acc, vali_loss, vali_acc])\n",
    "    if epoch == num_epochs - 1:\n",
    "      k_fold_val_acc.append(vali_acc)\n",
    "  # 测试\n",
    "  _, test_acc = evaluate(model, test_loader, crit, device)\n",
    "  print('测试集正确率：', test_acc)\n",
    "  k_fold_test_acc.append(test_acc)\n",
    "  print('测试集正确率：', test_acc)\n",
    "\n",
    "print('K-fold交叉验证，验证集的平均准确率：%.4f, 测试集的平均准确率：%.4f' % (sum(k_fold_val_acc)/len(k_fold_val_acc),sum(k_fold_test_acc)/len(k_fold_test_acc)))\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOUOrprXECqZ",
    "outputId": "1e9ebb19-9241-4f7a-add5-77b09bf220fb"
   },
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "交叉验证第0 / 5 轮训练\n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮，训练集损失：-0.23, 训练集准确率：0.60, 验证集损失：-0.42, 验证集准确率: 0.64\n",
      "第1轮，训练集损失：-0.50, 训练集准确率：0.63, 验证集损失：-0.63, 验证集准确率: 0.69\n",
      "第2轮，训练集损失：-0.69, 训练集准确率：0.69, 验证集损失：-0.81, 验证集准确率: 0.75\n",
      "第3轮，训练集损失：-0.86, 训练集准确率：0.72, 验证集损失：-0.96, 验证集准确率: 0.78\n",
      "第4轮，训练集损失：-1.00, 训练集准确率：0.77, 验证集损失：-1.09, 验证集准确率: 0.79\n",
      "第5轮，训练集损失：-1.11, 训练集准确率：0.79, 验证集损失：-1.19, 验证集准确率: 0.82\n",
      "第6轮，训练集损失：-1.22, 训练集准确率：0.82, 验证集损失：-1.29, 验证集准确率: 0.83\n",
      "第7轮，训练集损失：-1.31, 训练集准确率：0.86, 验证集损失：-1.36, 验证集准确率: 0.80\n",
      "第8轮，训练集损失：-1.40, 训练集准确率：0.88, 验证集损失：-1.45, 验证集准确率: 0.84\n",
      "第9轮，训练集损失：-1.48, 训练集准确率：0.90, 验证集损失：-1.52, 验证集准确率: 0.83\n",
      "第10轮，训练集损失：-1.56, 训练集准确率：0.91, 验证集损失：-1.57, 验证集准确率: 0.81\n",
      "第11轮，训练集损失：-1.63, 训练集准确率：0.93, 验证集损失：-1.65, 验证集准确率: 0.83\n",
      "第12轮，训练集损失：-1.70, 训练集准确率：0.94, 验证集损失：-1.71, 验证集准确率: 0.83\n",
      "第13轮，训练集损失：-1.77, 训练集准确率：0.94, 验证集损失：-1.77, 验证集准确率: 0.84\n",
      "第14轮，训练集损失：-1.84, 训练集准确率：0.95, 验证集损失：-1.81, 验证集准确率: 0.82\n",
      "第15轮，训练集损失：-1.90, 训练集准确率：0.95, 验证集损失：-1.87, 验证集准确率: 0.82\n",
      "第16轮，训练集损失：-1.96, 训练集准确率：0.95, 验证集损失：-1.92, 验证集准确率: 0.83\n",
      "第17轮，训练集损失：-2.02, 训练集准确率：0.96, 验证集损失：-1.98, 验证集准确率: 0.83\n",
      "第18轮，训练集损失：-2.08, 训练集准确率：0.96, 验证集损失：-1.99, 验证集准确率: 0.79\n",
      "第19轮，训练集损失：-2.14, 训练集准确率：0.97, 验证集损失：-2.07, 验证集准确率: 0.83\n",
      "第20轮，训练集损失：-2.19, 训练集准确率：0.97, 验证集损失：-2.11, 验证集准确率: 0.82\n",
      "第21轮，训练集损失：-2.25, 训练集准确率：0.97, 验证集损失：-2.18, 验证集准确率: 0.84\n",
      "第22轮，训练集损失：-2.30, 训练集准确率：0.97, 验证集损失：-2.23, 验证集准确率: 0.84\n",
      "第23轮，训练集损失：-2.35, 训练集准确率：0.97, 验证集损失：-2.26, 验证集准确率: 0.83\n",
      "第24轮，训练集损失：-2.40, 训练集准确率：0.97, 验证集损失：-2.31, 验证集准确率: 0.83\n",
      "第25轮，训练集损失：-2.45, 训练集准确率：0.97, 验证集损失：-2.37, 验证集准确率: 0.84\n",
      "第26轮，训练集损失：-2.50, 训练集准确率：0.97, 验证集损失：-2.43, 验证集准确率: 0.85\n",
      "第27轮，训练集损失：-2.55, 训练集准确率：0.97, 验证集损失：-2.46, 验证集准确率: 0.84\n",
      "第28轮，训练集损失：-2.60, 训练集准确率：0.97, 验证集损失：-2.50, 验证集准确率: 0.84\n",
      "第29轮，训练集损失：-2.65, 训练集准确率：0.97, 验证集损失：-2.55, 验证集准确率: 0.84\n",
      "第30轮，训练集损失：-2.70, 训练集准确率：0.97, 验证集损失：-2.60, 验证集准确率: 0.84\n",
      "第31轮，训练集损失：-2.75, 训练集准确率：0.98, 验证集损失：-2.61, 验证集准确率: 0.82\n",
      "第32轮，训练集损失：-2.79, 训练集准确率：0.97, 验证集损失：-2.70, 验证集准确率: 0.85\n",
      "第33轮，训练集损失：-2.84, 训练集准确率：0.97, 验证集损失：-2.75, 验证集准确率: 0.85\n",
      "第34轮，训练集损失：-2.90, 训练集准确率：0.98, 验证集损失：-2.76, 验证集准确率: 0.83\n",
      "第35轮，训练集损失：-2.95, 训练集准确率：0.98, 验证集损失：-2.77, 验证集准确率: 0.80\n",
      "第36轮，训练集损失：-2.99, 训练集准确率：0.98, 验证集损失：-2.80, 验证集准确率: 0.79\n",
      "第37轮，训练集损失：-3.04, 训练集准确率：0.98, 验证集损失：-2.91, 验证集准确率: 0.84\n",
      "第38轮，训练集损失：-3.08, 训练集准确率：0.98, 验证集损失：-2.97, 验证集准确率: 0.85\n",
      "第39轮，训练集损失：-3.13, 训练集准确率：0.98, 验证集损失：-3.00, 验证集准确率: 0.84\n",
      "第40轮，训练集损失：-3.17, 训练集准确率：0.98, 验证集损失：-3.04, 验证集准确率: 0.85\n",
      "第41轮，训练集损失：-3.22, 训练集准确率：0.98, 验证集损失：-3.02, 验证集准确率: 0.79\n",
      "第42轮，训练集损失：-3.25, 训练集准确率：0.97, 验证集损失：-3.13, 验证集准确率: 0.84\n",
      "第43轮，训练集损失：-3.32, 训练集准确率：0.99, 验证集损失：-3.17, 验证集准确率: 0.84\n",
      "第44轮，训练集损失：-3.35, 训练集准确率：0.98, 验证集损失：-3.22, 验证集准确率: 0.85\n",
      "第45轮，训练集损失：-3.39, 训练集准确率：0.97, 验证集损失：-3.25, 验证集准确率: 0.84\n",
      "第46轮，训练集损失：-3.45, 训练集准确率：0.98, 验证集损失：-3.30, 验证集准确率: 0.84\n",
      "第47轮，训练集损失：-3.50, 训练集准确率：0.98, 验证集损失：-3.33, 验证集准确率: 0.83\n",
      "第48轮，训练集损失：-3.53, 训练集准确率：0.98, 验证集损失：-3.33, 验证集准确率: 0.80\n",
      "第49轮，训练集损失：-3.57, 训练集准确率：0.97, 验证集损失：-3.41, 验证集准确率: 0.83\n",
      "测试集正确率： 0.817966903073286\n",
      "测试集正确率： 0.817966903073286\n",
      "\n",
      "交叉验证第1 / 5 轮训练\n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮，训练集损失：-0.24, 训练集准确率：0.57, 验证集损失：-0.42, 验证集准确率: 0.57\n",
      "第1轮，训练集损失：-0.51, 训练集准确率：0.61, 验证集损失：-0.65, 验证集准确率: 0.62\n",
      "第2轮，训练集损失：-0.71, 训练集准确率：0.68, 验证集损失：-0.83, 验证集准确率: 0.71\n",
      "第3轮，训练集损失：-0.88, 训练集准确率：0.73, 验证集损失：-0.99, 验证集准确率: 0.70\n",
      "第4轮，训练集损失：-1.03, 训练集准确率：0.76, 验证集损失：-1.12, 验证集准确率: 0.73\n",
      "第5轮，训练集损失：-1.15, 训练集准确率：0.80, 验证集损失：-1.23, 验证集准确率: 0.77\n",
      "第6轮，训练集损失：-1.26, 训练集准确率：0.83, 验证集损失：-1.33, 验证集准确率: 0.80\n",
      "第7轮，训练集损失：-1.36, 训练集准确率：0.86, 验证集损失：-1.42, 验证集准确率: 0.83\n",
      "第8轮，训练集损失：-1.45, 训练集准确率：0.87, 验证集损失：-1.49, 验证集准确率: 0.81\n",
      "第9轮，训练集损失：-1.53, 训练集准确率：0.88, 验证集损失：-1.56, 验证集准确率: 0.82\n",
      "第10轮，训练集损失：-1.62, 训练集准确率：0.91, 验证集损失：-1.65, 验证集准确率: 0.85\n",
      "第11轮，训练集损失：-1.71, 训练集准确率：0.92, 验证集损失：-1.71, 验证集准确率: 0.84\n",
      "第12轮，训练集损失：-1.78, 训练集准确率：0.93, 验证集损失：-1.78, 验证集准确率: 0.85\n",
      "第13轮，训练集损失：-1.85, 训练集准确率：0.93, 验证集损失：-1.85, 验证集准确率: 0.86\n",
      "第14轮，训练集损失：-1.93, 训练集准确率：0.95, 验证集损失：-1.90, 验证集准确率: 0.85\n",
      "第15轮，训练集损失：-1.99, 训练集准确率：0.95, 验证集损失：-1.96, 验证集准确率: 0.87\n",
      "第16轮，训练集损失：-2.05, 训练集准确率：0.94, 验证集损失：-2.01, 验证集准确率: 0.85\n",
      "第17轮，训练集损失：-2.12, 训练集准确率：0.95, 验证集损失：-2.05, 验证集准确率: 0.85\n",
      "第18轮，训练集损失：-2.17, 训练集准确率：0.95, 验证集损失：-2.12, 验证集准确率: 0.86\n",
      "第19轮，训练集损失：-2.23, 训练集准确率：0.95, 验证集损失：-2.18, 验证集准确率: 0.87\n",
      "第20轮，训练集损失：-2.29, 训练集准确率：0.96, 验证集损失：-2.20, 验证集准确率: 0.84\n",
      "第21轮，训练集损失：-2.35, 训练集准确率：0.96, 验证集损失：-2.27, 验证集准确率: 0.86\n",
      "第22轮，训练集损失：-2.40, 训练集准确率：0.96, 验证集损失：-2.31, 验证集准确率: 0.86\n",
      "第23轮，训练集损失：-2.44, 训练集准确率：0.95, 验证集损失：-2.33, 验证集准确率: 0.82\n",
      "第24轮，训练集损失：-2.49, 训练集准确率：0.95, 验证集损失：-2.41, 验证集准确率: 0.86\n",
      "第25轮，训练集损失：-2.54, 训练集准确率：0.96, 验证集损失：-2.34, 验证集准确率: 0.76\n",
      "第26轮，训练集损失：-2.59, 训练集准确率：0.95, 验证集损失：-2.50, 验证集准确率: 0.85\n",
      "第27轮，训练集损失：-2.65, 训练集准确率：0.97, 验证集损失：-2.54, 验证集准确率: 0.85\n",
      "第28轮，训练集损失：-2.69, 训练集准确率：0.96, 验证集损失：-2.54, 验证集准确率: 0.82\n",
      "第29轮，训练集损失：-2.72, 训练集准确率：0.95, 验证集损失：-2.62, 验证集准确率: 0.84\n",
      "第30轮，训练集损失：-2.80, 训练集准确率：0.96, 验证集损失：-2.68, 验证集准确率: 0.85\n",
      "第31轮，训练集损失：-2.84, 训练集准确率：0.96, 验证集损失：-2.72, 验证集准确率: 0.85\n",
      "第32轮，训练集损失：-2.89, 训练集准确率：0.97, 验证集损失：-2.80, 验证集准确率: 0.87\n",
      "第33轮，训练集损失：-2.94, 训练集准确率：0.96, 验证集损失：-2.84, 验证集准确率: 0.87\n",
      "第34轮，训练集损失：-2.98, 训练集准确率：0.97, 验证集损失：-2.83, 验证集准确率: 0.84\n",
      "第35轮，训练集损失：-3.03, 训练集准确率：0.97, 验证集损失：-2.87, 验证集准确率: 0.83\n",
      "第36轮，训练集损失：-3.08, 训练集准确率：0.96, 验证集损失：-2.93, 验证集准确率: 0.84\n",
      "第37轮，训练集损失：-3.13, 训练集准确率：0.97, 验证集损失：-2.98, 验证集准确率: 0.85\n",
      "第38轮，训练集损失：-3.18, 训练集准确率：0.97, 验证集损失：-3.05, 验证集准确率: 0.87\n",
      "第39轮，训练集损失：-3.21, 训练集准确率：0.96, 验证集损失：-3.10, 验证集准确率: 0.87\n",
      "第40轮，训练集损失：-3.27, 训练集准确率：0.97, 验证集损失：-3.16, 验证集准确率: 0.88\n",
      "第41轮，训练集损失：-3.32, 训练集准确率：0.97, 验证集损失：-3.11, 验证集准确率: 0.82\n",
      "第42轮，训练集损失：-3.33, 训练集准确率：0.95, 验证集损失：-3.12, 验证集准确率: 0.80\n",
      "第43轮，训练集损失：-3.42, 训练集准确率：0.98, 验证集损失：-3.28, 验证集准确率: 0.87\n",
      "第44轮，训练集损失：-3.46, 训练集准确率：0.97, 验证集损失：-3.24, 验证集准确率: 0.82\n",
      "第45轮，训练集损失：-3.46, 训练集准确率：0.95, 验证集损失：-3.14, 验证集准确率: 0.75\n",
      "第46轮，训练集损失：-3.51, 训练集准确率：0.95, 验证集损失：-3.38, 验证集准确率: 0.86\n",
      "第47轮，训练集损失：-3.56, 训练集准确率：0.95, 验证集损失：-3.33, 验证集准确率: 0.80\n",
      "第48轮，训练集损失：-3.61, 训练集准确率：0.96, 验证集损失：-3.27, 验证集准确率: 0.75\n",
      "第49轮，训练集损失：-3.64, 训练集准确率：0.95, 验证集损失：-3.53, 验证集准确率: 0.86\n",
      "测试集正确率： 0.8250591016548463\n",
      "测试集正确率： 0.8250591016548463\n",
      "\n",
      "交叉验证第2 / 5 轮训练\n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0轮，训练集损失：-0.23, 训练集准确率：0.57, 验证集损失：-0.43, 验证集准确率: 0.55\n",
      "第1轮，训练集损失：-0.51, 训练集准确率：0.57, 验证集损失：-0.64, 验证集准确率: 0.61\n",
      "第2轮，训练集损失：-0.70, 训练集准确率：0.60, 验证集损失：-0.81, 验证集准确率: 0.59\n",
      "第3轮，训练集损失：-0.87, 训练集准确率：0.64, 验证集损失：-0.96, 验证集准确率: 0.64\n",
      "第4轮，训练集损失：-1.01, 训练集准确率：0.70, 验证集损失：-1.08, 验证集准确率: 0.73\n",
      "第5轮，训练集损失：-1.13, 训练集准确率：0.76, 验证集损失：-1.18, 验证集准确率: 0.69\n",
      "第6轮，训练集损失：-1.24, 训练集准确率：0.83, 验证集损失：-1.28, 验证集准确率: 0.76\n",
      "第7轮，训练集损失：-1.33, 训练集准确率：0.85, 验证集损失：-1.37, 验证集准确率: 0.78\n",
      "第8轮，训练集损失：-1.42, 训练集准确率：0.87, 验证集损失：-1.45, 验证集准确率: 0.79\n",
      "第9轮，训练集损失：-1.51, 训练集准确率：0.90, 验证集损失：-1.52, 验证集准确率: 0.79\n",
      "第10轮，训练集损失：-1.59, 训练集准确率：0.91, 验证集损失：-1.55, 验证集准确率: 0.75\n",
      "第11轮，训练集损失：-1.66, 训练集准确率：0.93, 验证集损失：-1.65, 验证集准确率: 0.81\n",
      "第12轮，训练集损失：-1.74, 训练集准确率：0.93, 验证集损失：-1.70, 验证集准确率: 0.79\n",
      "第13轮，训练集损失：-1.81, 训练集准确率：0.93, 验证集损失：-1.76, 验证集准确率: 0.79\n",
      "第14轮，训练集损失：-1.88, 训练集准确率：0.94, 验证集损失：-1.81, 验证集准确率: 0.80\n",
      "第15轮，训练集损失：-1.95, 训练集准确率：0.96, 验证集损失：-1.85, 验证集准确率: 0.79\n",
      "第16轮，训练集损失：-2.01, 训练集准确率：0.96, 验证集损失：-1.91, 验证集准确率: 0.79\n",
      "第17轮，训练集损失：-2.06, 训练集准确率：0.96, 验证集损失：-1.98, 验证集准确率: 0.81\n",
      "第18轮，训练集损失：-2.13, 训练集准确率：0.97, 验证集损失：-2.01, 验证集准确率: 0.78\n",
      "第19轮，训练集损失：-2.18, 训练集准确率：0.97, 验证集损失：-2.08, 验证集准确率: 0.81\n",
      "第20轮，训练集损失：-2.23, 训练集准确率：0.97, 验证集损失：-2.11, 验证集准确率: 0.79\n",
      "第21轮，训练集损失：-2.28, 训练集准确率：0.96, 验证集损失：-2.14, 验证集准确率: 0.78\n",
      "第22轮，训练集损失：-2.34, 训练集准确率：0.97, 验证集损失：-2.21, 验证集准确率: 0.81\n",
      "第23轮，训练集损失：-2.39, 训练集准确率：0.97, 验证集损失：-2.24, 验证集准确率: 0.79\n",
      "第24轮，训练集损失：-2.45, 训练集准确率：0.97, 验证集损失：-2.30, 验证集准确率: 0.80\n",
      "第25轮，训练集损失：-2.49, 训练集准确率：0.97, 验证集损失：-2.33, 验证集准确率: 0.79\n",
      "第26轮，训练集损失：-2.55, 训练集准确率：0.97, 验证集损失：-2.38, 验证集准确率: 0.79\n",
      "第27轮，训练集损失：-2.59, 训练集准确率：0.97, 验证集损失：-2.43, 验证集准确率: 0.80\n",
      "第28轮，训练集损失：-2.64, 训练集准确率：0.97, 验证集损失：-2.50, 验证集准确率: 0.82\n",
      "第29轮，训练集损失：-2.69, 训练集准确率：0.97, 验证集损失：-2.53, 验证集准确率: 0.81\n",
      "第30轮，训练集损失：-2.75, 训练集准确率：0.98, 验证集损失：-2.56, 验证集准确率: 0.79\n",
      "第31轮，训练集损失：-2.79, 训练集准确率：0.97, 验证集损失：-2.61, 验证集准确率: 0.80\n",
      "第32轮，训练集损失：-2.83, 训练集准确率：0.97, 验证集损失：-2.65, 验证集准确率: 0.79\n",
      "第33轮，训练集损失：-2.89, 训练集准确率：0.98, 验证集损失：-2.71, 验证集准确率: 0.81\n",
      "第34轮，训练集损失：-2.93, 训练集准确率：0.98, 验证集损失：-2.73, 验证集准确率: 0.79\n",
      "第35轮，训练集损失：-2.97, 训练集准确率：0.97, 验证集损失：-2.78, 验证集准确率: 0.80\n",
      "第36轮，训练集损失：-3.02, 训练集准确率：0.97, 验证集损失：-2.83, 验证集准确率: 0.80\n",
      "第37轮，训练集损失：-3.08, 训练集准确率：0.98, 验证集损失：-2.85, 验证集准确率: 0.79\n",
      "第38轮，训练集损失：-3.12, 训练集准确率：0.98, 验证集损失：-2.90, 验证集准确率: 0.79\n",
      "第39轮，训练集损失：-3.18, 训练集准确率：0.98, 验证集损失：-2.95, 验证集准确率: 0.80\n",
      "第40轮，训练集损失：-3.22, 训练集准确率：0.98, 验证集损失：-2.98, 验证集准确率: 0.79\n",
      "第41轮，训练集损失：-3.26, 训练集准确率：0.98, 验证集损失：-3.05, 验证集准确率: 0.80\n",
      "第42轮，训练集损失：-3.31, 训练集准确率：0.98, 验证集损失：-3.06, 验证集准确率: 0.79\n",
      "第43轮，训练集损失：-3.35, 训练集准确率：0.98, 验证集损失：-3.13, 验证集准确率: 0.80\n",
      "第44轮，训练集损失：-3.38, 训练集准确率：0.96, 验证集损失：-3.17, 验证集准确率: 0.80\n",
      "第45轮，训练集损失：-3.45, 训练集准确率：0.98, 验证集损失：-3.23, 验证集准确率: 0.80\n",
      "第46轮，训练集损失：-3.47, 训练集准确率：0.97, 验证集损失：-3.26, 验证集准确率: 0.80\n",
      "第47轮，训练集损失：-3.54, 训练集准确率：0.98, 验证集损失：-3.33, 验证集准确率: 0.82\n",
      "第48轮，训练集损失：-3.57, 训练集准确率：0.97, 验证集损失：-3.35, 验证集准确率: 0.80\n",
      "第49轮，训练集损失：-3.63, 训练集准确率：0.98, 验证集损失：-3.39, 验证集准确率: 0.80\n",
      "测试集正确率： 0.8309692671394799\n",
      "测试集正确率： 0.8309692671394799\n",
      "\n",
      "交叉验证第3 / 5 轮训练\n",
      "\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "第0轮，训练集损失：-0.23, 训练集准确率：0.66, 验证集损失：-0.41, 验证集准确率: 0.68\n",
      "第1轮，训练集损失：-0.49, 训练集准确率：0.73, 验证集损失：-0.62, 验证集准确率: 0.72\n",
      "第2轮，训练集损失：-0.68, 训练集准确率：0.77, 验证集损失：-0.79, 验证集准确率: 0.72\n",
      "第3轮，训练集损失：-0.84, 训练集准确率：0.79, 验证集损失：-0.94, 验证集准确率: 0.80\n",
      "第4轮，训练集损失：-0.98, 训练集准确率：0.81, 验证集损失：-1.07, 验证集准确率: 0.80\n",
      "第5轮，训练集损失：-1.10, 训练集准确率：0.84, 验证集损失：-1.18, 验证集准确率: 0.82\n",
      "第6轮，训练集损失：-1.22, 训练集准确率：0.88, 验证集损失：-1.27, 验证集准确率: 0.81\n",
      "第7轮，训练集损失：-1.32, 训练集准确率：0.89, 验证集损失：-1.37, 验证集准确率: 0.83\n",
      "第8轮，训练集损失：-1.42, 训练集准确率：0.90, 验证集损失：-1.45, 验证集准确率: 0.84\n",
      "第9轮，训练集损失：-1.51, 训练集准确率：0.91, 验证集损失：-1.52, 验证集准确率: 0.83\n",
      "第10轮，训练集损失：-1.60, 训练集准确率：0.93, 验证集损失：-1.59, 验证集准确率: 0.83\n",
      "第11轮，训练集损失：-1.68, 训练集准确率：0.93, 验证集损失：-1.66, 验证集准确率: 0.84\n",
      "第12轮，训练集损失：-1.75, 训练集准确率：0.94, 验证集损失：-1.69, 验证集准确率: 0.81\n",
      "第13轮，训练集损失：-1.82, 训练集准确率：0.94, 验证集损失：-1.77, 验证集准确率: 0.83\n",
      "第14轮，训练集损失：-1.89, 训练集准确率：0.96, 验证集损失：-1.83, 验证集准确率: 0.84\n",
      "第15轮，训练集损失：-1.96, 训练集准确率：0.96, 验证集损失：-1.88, 验证集准确率: 0.84\n",
      "第16轮，训练集损失：-2.02, 训练集准确率：0.95, 验证集损失：-1.94, 验证集准确率: 0.85\n",
      "第17轮，训练集损失：-2.08, 训练集准确率：0.96, 验证集损失：-1.99, 验证集准确率: 0.84\n",
      "第18轮，训练集损失：-2.13, 训练集准确率：0.96, 验证集损失：-2.03, 验证集准确率: 0.84\n",
      "第19轮，训练集损失：-2.20, 训练集准确率：0.96, 验证集损失：-2.09, 验证集准确率: 0.85\n",
      "第20轮，训练集损失：-2.25, 训练集准确率：0.96, 验证集损失：-2.12, 验证集准确率: 0.83\n",
      "第21轮，训练集损失：-2.31, 训练集准确率：0.97, 验证集损失：-2.19, 验证集准确率: 0.85\n",
      "第22轮，训练集损失：-2.36, 训练集准确率：0.97, 验证集损失：-2.23, 验证集准确率: 0.84\n",
      "第23轮，训练集损失：-2.42, 训练集准确率：0.98, 验证集损失：-2.28, 验证集准确率: 0.84\n",
      "第24轮，训练集损失：-2.45, 训练集准确率：0.96, 验证集损失：-2.33, 验证集准确率: 0.85\n",
      "第25轮，训练集损失：-2.52, 训练集准确率：0.97, 验证集损失：-2.34, 验证集准确率: 0.83\n",
      "第26轮，训练集损失：-2.56, 训练集准确率：0.97, 验证集损失：-2.37, 验证集准确率: 0.82\n",
      "第27轮，训练集损失：-2.61, 训练集准确率：0.97, 验证集损失：-2.45, 验证集准确率: 0.84\n",
      "第28轮，训练集损失：-2.66, 训练集准确率：0.97, 验证集损失：-2.50, 验证集准确率: 0.85\n",
      "第29轮，训练集损失：-2.71, 训练集准确率：0.97, 验证集损失：-2.56, 验证集准确率: 0.85\n",
      "第30轮，训练集损失：-2.76, 训练集准确率：0.97, 验证集损失：-2.61, 验证集准确率: 0.86\n",
      "第31轮，训练集损失：-2.81, 训练集准确率：0.97, 验证集损失：-2.64, 验证集准确率: 0.85\n",
      "第32轮，训练集损失：-2.85, 训练集准确率：0.97, 验证集损失：-2.70, 验证集准确率: 0.86\n",
      "第33轮，训练集损失：-2.91, 训练集准确率：0.97, 验证集损失：-2.72, 验证集准确率: 0.84\n",
      "第34轮，训练集损失：-2.93, 训练集准确率：0.96, 验证集损失：-2.75, 验证集准确率: 0.84\n",
      "第35轮，训练集损失：-2.99, 训练集准确率：0.97, 验证集损失：-2.83, 验证集准确率: 0.86\n",
      "第36轮，训练集损失：-3.04, 训练集准确率：0.97, 验证集损失：-2.74, 验证集准确率: 0.79\n",
      "第37轮，训练集损失：-3.09, 训练集准确率：0.97, 验证集损失：-2.87, 验证集准确率: 0.83\n",
      "第38轮，训练集损失：-3.15, 训练集准确率：0.98, 验证集损失：-2.93, 验证集准确率: 0.84\n",
      "第39轮，训练集损失：-3.19, 训练集准确率：0.98, 验证集损失：-2.98, 验证集准确率: 0.84\n",
      "第40轮，训练集损失：-3.23, 训练集准确率：0.97, 验证集损失：-3.02, 验证集准确率: 0.84\n",
      "第41轮，训练集损失：-3.27, 训练集准确率：0.97, 验证集损失：-3.04, 验证集准确率: 0.83\n",
      "第42轮，训练集损失：-3.32, 训练集准确率：0.97, 验证集损失：-3.12, 验证集准确率: 0.84\n",
      "第43轮，训练集损失：-3.38, 训练集准确率：0.98, 验证集损失：-3.17, 验证集准确率: 0.85\n",
      "第44轮，训练集损失：-3.41, 训练集准确率：0.97, 验证集损失：-3.22, 验证集准确率: 0.85\n",
      "第45轮，训练集损失：-3.46, 训练集准确率：0.97, 验证集损失：-3.27, 验证集准确率: 0.86\n",
      "第46轮，训练集损失：-3.47, 训练集准确率：0.96, 验证集损失：-3.30, 验证集准确率: 0.85\n",
      "第47轮，训练集损失：-3.56, 训练集准确率：0.98, 验证集损失：-3.28, 验证集准确率: 0.83\n",
      "第48轮，训练集损失：-3.59, 训练集准确率：0.97, 验证集损失：-3.36, 验证集准确率: 0.85\n",
      "第49轮，训练集损失：-3.63, 训练集准确率：0.97, 验证集损失：-3.40, 验证集准确率: 0.84\n",
      "测试集正确率： 0.8167848699763594\n",
      "测试集正确率： 0.8167848699763594\n",
      "\n",
      "交叉验证第4 / 5 轮训练\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "第0轮，训练集损失：-0.24, 训练集准确率：0.60, 验证集损失：-0.42, 验证集准确率: 0.74\n",
      "第1轮，训练集损失：-0.51, 训练集准确率：0.71, 验证集损失：-0.64, 验证集准确率: 0.75\n",
      "第2轮，训练集损失：-0.71, 训练集准确率：0.73, 验证集损失：-0.83, 验证集准确率: 0.79\n",
      "第3轮，训练集损失：-0.88, 训练集准确率：0.76, 验证集损失：-0.99, 验证集准确率: 0.79\n",
      "第4轮，训练集损失：-1.02, 训练集准确率：0.78, 验证集损失：-1.11, 验证集准确率: 0.80\n",
      "第5轮，训练集损失：-1.15, 训练集准确率：0.81, 验证集损失：-1.22, 验证集准确率: 0.81\n",
      "第6轮，训练集损失：-1.25, 训练集准确率：0.83, 验证集损失：-1.32, 验证集准确率: 0.82\n",
      "第7轮，训练集损失：-1.34, 训练集准确率：0.85, 验证集损失：-1.40, 验证集准确率: 0.83\n",
      "第8轮，训练集损失：-1.43, 训练集准确率：0.87, 验证集损失：-1.48, 验证集准确率: 0.83\n",
      "第9轮，训练集损失：-1.51, 训练集准确率：0.90, 验证集损失：-1.56, 验证集准确率: 0.84\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e009a682218d944d4ca170df67ae304433d3da7f548affd1bafd64b5b995fea"
   }
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "w-jxVm_yZ0TI",
    "h470aVfAZ0TL",
    "9VFN-lVbZ0TM",
    "sO3_QvSiZ0TO"
   ],
   "toc_visible": true,
   "gpuType": "V28"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
